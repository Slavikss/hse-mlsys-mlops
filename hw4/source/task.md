Дедлайн: 23:59 20 ноября 

Для выполнения домашнего задания рекомендуется изучить практическую часть с лекции и семинара №7. 

Вам необходимо научиться работать с Clickhouse, для этого нужно загрузить данные из топика Kafka в Clickhouse, написать sql запрос, оптимизировать хранение таблицы

Данные находятся тут:
https://www.kaggle.com/competitions/teta-ml-1-2025/data?select=train.csv


Требования к реализации:
1.	Нужно загрузить данные из csv файла в топик Kafka (должен быть отдельный py скрипт)
2.	Сформировать таблицы в Clickhouse получающие данные из топика в таблицу (отдельный sql скрипт содержащий DLL таблиц)
3.	Сформировать sql запрос для получения категории наибольшей транзакции по каждому штату (отдельный sql запрос + результирующий файл csv)
4.	Оптимизировать хранение данных чтобы запросы испольнялись быстрее (изменить DLL из п.2)

Необходимо оформить в виде Github репозитория с оформленным Readme файлом и подготовленными requirements и docker-compose.yml для сборки docker images и поднятия контейнеров
По инструкции из Readme файла у меня не должно возникать проблем с поднятием контейнеров проекта и теста его работоспособности

Оценивание работы

Оценка 6-7 баллов:
Выполнены п. 1-3

Оценка 8-10 баллов:
	Выполнен п. 4

Как отправить результат:
В качестве решения необходимо предоставить ссылку на код проекта, загруженного на GitHub. Репозиторий должен быть публичным, чтобы мы могли без проблем сделать pull.
Желаем удачи!

